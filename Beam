import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions
from apache_beam.io.gcp.bigquery import ReadFromBigQuery
from apache_beam.io.parquetio import WriteToParquet
from datetime import date
import json
import os

class CustomOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_argument('--output_path', required=True, help='Path to write output data')

def format_clearance_item(row):
    return {
        'startDate': row['effectivedate'],
        'endDate': row['expirationdate'],
        'basePrice': row['originalamount'],
        'discountValue': row['discountedamount'],
        'itemId': row['itemnbr'],
        'clubs': [int(row['clubnbr'])],
        'timeZone': 'UTC',
        'savingsId': f"{row['clubnbr']}{row['itemnbr']}",
        'savingsType': 'Clearance',
        'applicableChannels': [],
        'discountType': 'AMOUNT_OFF',
        'eventTag': 0,
        'members': [],
        'items': "abc,DiscountedItem,xyz",
        'clubOverrides': ",,",
        'productId': None
    }

def create_items_field(row):
    item_schema = {
        'itemId': row['itemId'],
        'productId': row['productId'],
        'itemType': 'DiscountedItem',
        'productItemMappingStatus': row['productItemMappingStatus']
    }
    row['items'] = [item_schema]
    return row

def create_club_overrides_field(row):
    club_overrides_schema = {
        'clubNumber': 0,
        'clubStartDate': '',
        'clubEndDate': ''
    }
    row['clubOverrides'] = [club_overrides_schema]
    return row

def run(argv=None):
    options = PipelineOptions(argv)
    custom_options = options.view_as(CustomOptions)
    custom_options.view_as(StandardOptions).runner = 'DirectRunner'

    query_clearance_items = """
    SELECT t2.effectivedate,t2.expirationdate, t1.retailamount as originalamount, t1.retailamount-t2.retailamount as discountedamount, t1.itemnbr, t1.clubnbr
            FROM `prod-sams-cdp.prod_pricing_wingman_pricing.current_retail_action` t1
            JOIN `prod-sams-cdp.prod_pricing_wingman_pricing.current_retail_action` t2
            ON t1.itemnbr=t2.itemnbr
            WHERE t1.retailtype ="BP" and t1.clubnbr = 6279 and t2.retailtype ="MD" and t2.clubnbr = 6279 and DATE(t2.effectivedate) <= CURRENT_DATE() and DATE(t2.expirationdate) >= CURRENT_DATE() and t1.retailamount-t2.retailamount > 0
    """

    query_cdp_items = """
    select t1.PROD_ID, t1.ITEM_NBR FROM `prod-sams-cdp.US_SAMS_PRODUCT360_CDP_VM.CLUB_ITEM_GRP` t1
                join `prod-sams-cdp.US_SAMS_PRODUCT360_CDP_VM.PROD` t2
                on t1.PROD_ID = t2.PROD_ID
                where t2.PROD_STATUS_CD = 'ACTIVE'
    """

    with beam.Pipeline(options=options) as p:
        # Read clearance items
        clearance_items_metadata = (p 
                                    | 'Read Clearance Items' >> beam.io.ReadFromBigQuery(query=query_clearance_items, use_standard_sql=True)
                                    | 'Format Clearance Items' >> beam.Map(format_clearance_item)
        )

        # Read CDP items
        cdp_items_list = (p 
                          | 'Read CDP Items' >> beam.io.ReadFromBigQuery(query=query_cdp_items, use_standard_sql=True)
                          | 'Extract Key' >> beam.Map(lambda row: (row['ITEM_NBR'], row))
        )

        # Join clearance items with CDP items
        joined_clearance_items = ({
            'clearance': clearance_items_metadata, 
            'cdp': cdp_items_list
        }
        | 'CoGroupByKey' >> beam.CoGroupByKey()
        | 'Join Clearance and CDP Items' >> beam.FlatMap(
            lambda kv: [
                dict(clearance_item, **{'productId': cdp_item['PROD_ID']}) 
                for clearance_item in kv[1]['clearance']
                for cdp_item in kv[1]['cdp']
            ]
        )
        )

        # Count distinct products for each item
        cdp_items_list_grouped = (cdp_items_list 
                                  | 'Group by Item ID' >> beam.GroupByKey()
                                  | 'Count Distinct Products' >> beam.Map(
                                      lambda kv: (kv[0], len(set(item['PROD_ID'] for item in kv[1])))
                                  )
        )

        # Join clearance items with grouped CDP items
        clearance_items_with_product_count = ({
            'clearance': joined_clearance_items, 
            'product_count': cdp_items_list_grouped
        }
        | 'CoGroupByKey Product Count' >> beam.CoGroupByKey()
        | 'Join with Product Count' >> beam.FlatMap(
            lambda kv: [
                dict(clearance_item, **{'ProductCount': count, 'productItemMappingStatus': (
                    '1-to-multiple' if count > 1
                    else 'missing' if clearance_item['productId'] is None
                    else 'normal'
                ), 'productId': clearance_item['productId'] if clearance_item['productId'] is not None else ''}) 
                for clearance_item in kv[1]['clearance']
                for count in kv[1]['product_count']
            ]
        )
        )

        # Final processing
        final_clearance_items = (clearance_items_with_product_count
                                 | 'Create Items Field' >> beam.Map(create_items_field)
                                 | 'Create Club Overrides Field' >> beam.Map(create_club_overrides_field)
        )

        # Write to Parquet on local drive (C: drive)
        output_path = custom_options.output_path
        final_clearance_items | 'Write to Parquet' >> WriteToParquet(
            file_path_prefix=os.path.join(output_path, str(date.today())),
            file_name_suffix='.parquet',
            schema=clearance_items_metadata_schema()
        )

def clearance_items_metadata_schema():
    schema_str = json.dumps({
        "fields": [
            {"name": "startDate", "type": "STRING", "mode": "NULLABLE"},
            {"name": "endDate", "type": "STRING", "mode": "NULLABLE"},
            {"name": "basePrice", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "discountValue", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "timeZone", "type": "STRING", "mode": "NULLABLE"},
            {"name": "savingsId", "type": "STRING", "mode": "NULLABLE"},
            {"name": "savingsType", "type": "STRING", "mode": "NULLABLE"},
            {"name": "applicableChannels", "type": "STRING", "mode": "REPEATED"},
            {"name": "discountType", "type": "STRING", "mode": "NULLABLE"},
            {"name": "eventTag", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "members", "type": "STRING", "mode": "REPEATED"},
            {"name": "items", "type": "RECORD", "mode": "REPEATED", "fields": [
                {"name": "itemId", "type": "STRING", "mode": "NULLABLE"},
                {"name": "productId", "type": "STRING", "mode": "NULLABLE"},
                {"name": "itemType", "type": "STRING", "mode": "NULLABLE"},
                {"name": "productItemMappingStatus", "type": "STRING", "mode": "NULLABLE"}
            ]},
            {"name": "clubOverrides", "type": "RECORD", "mode": "REPEATED", "fields": [
                {"name": "clubNumber", "type": "INTEGER", "mode": "NULLABLE"},
                {"name": "clubStartDate", "type": "STRING", "mode": "NULLABLE"},
                {"name": "clubEndDate", "type": "STRING", "mode": "NULLABLE"}
            ]},
            {"name": "clubs", "type": "INTEGER", "mode": "REPEATED"}
        ]
    })
    return parse_table_schema_from_json(schema_str)

def parse_table_schema_from_json(schema_json):
    from apache_beam.io.gcp.internal.clients import bigquery
    return bigquery.TableSchema(
        fields=[
            bigquery.TableFieldSchema(
                name=field['name'],
                type=field['type'],
                mode=field.get('mode', 'NULLABLE'),
                fields=parse_table_schema_from_json(field.get('fields', '')) if 'fields' in field else None
            )
            for field in json.loads(schema_json)['fields']
        ]
    )

# Run the pipeline
if __name__ == '__main__':
    import sys
    argv = [
        '--output_path', '/Users/vn53tqg/'
    ]
    run(argv)
